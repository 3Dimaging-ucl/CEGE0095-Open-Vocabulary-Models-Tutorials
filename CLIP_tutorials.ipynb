{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPvhFOXYi6SQJMISqfC1EVD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CLIP-based Open-Vocabulary Image Classification Tutorial\n","[June Moh Goo](https://www.linkedin.com/in/jmgoo1118/) / PhD Student in Computer Vision for 3D Point Clouds\n","\n","---\n","---\n","\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1i8ua1BmlfFGUm51QkngPFx_yahD15Tmb#scrollTo=Z5wZpN2EOIMR) [![Paper](https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg)](https://arxiv.org/pdf/2103.00020) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/OpenAI/CLIP)\n","\n","In this tutorial, we will demonstrate how to use the CLIP model proposed by OpenAI to perform open-vocabulary image classification. CLIP (Contrastive Language-Image Pre-training) aligns images and text in the same embedding space, enabling classification without fixed class labels, simply by providing textual prompts.\n","\n","## Key Idea\n","\n","The key idea of OpenAI's CLIP (Contrastive Language–Image Pre-training) is to create a shared embedding space for images and text, enabling mutual understanding between the two modalities. CLIP achieves this by using contrastive learning on a large dataset of image-text pairs.\n","\n","Main Concepts:\n","- Image-Text Matching: CLIP learns to align images and their corresponding textual descriptions by maximizing their similarity in the shared embedding space.\n","- Multimodal Representation Learning: It uses two separate encoders (a Vision Transformer for images and a Transformer for text) to process each modality and project them into a common embedding space.\n","- Zero-Shot Learning: After training, CLIP can generalize to new tasks and datasets without additional fine-tuning, allowing text descriptions to classify images directly.\n","\n","CLIP's strength lies in its generalization capability, performing at human-level accuracy across various datasets and tasks without task-specific training.\n","\n","For example, CLIP can identify a picture of a cat without being explicitly trained on a \"cat classification\" dataset. Instead, you can provide text prompts like \"a photo of a cat\", \"a photo of a dog\", and \"a photo of a car\", and it will correctly associate the cat image with the corresponding text description based on its pre-trained image-text alignment. This flexibility allows CLIP to classify or describe images for tasks it wasn’t specifically trained for, such as identifying objects, scenes, or even abstract concepts, simply by providing appropriate textual labels.\n","\n","\n","![CLIP figure](https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true)\n","\n","## What You Will Learn\n","\n","- How to install and load the CLIP model in Google Colab\n","- How to load and preprocess an example image\n","- How to create image and text embeddings using CLIP\n","- How to compare image embeddings against multiple textual prompts to find the best match\n","\n","## Requirements\n","\n","- Google Colab environment (GPU recommended)\n","- `torch`, `clip` libraries\n","- Internet connection (to download example images)\n","\n","## References\n","\n","- [OpenAI CLIP GitHub Repository](https://github.com/openai/CLIP)\n","- [Hugging Face Transformers (CLIP)](https://huggingface.co/docs/transformers/model_doc/clip)\n"],"metadata":{"id":"Z5wZpN2EOIMR"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"1Q73UObbOGRM","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1734349716663,"user_tz":0,"elapsed":2603,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}},"outputId":"fefb79fc-8d02-4ed8-afc2-e3dd0e3c3b6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["# Check runtime environment (GPU recommended)\n","import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device\n"]},{"cell_type":"markdown","source":["## Installation and Imports\n","\n","Let's install CLIP and import the necessary libraries.\n"],"metadata":{"id":"O3UT1JeJOpfV"}},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git --upgrade ftfy regex tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMBFsbN7OYYO","executionInfo":{"status":"ok","timestamp":1734349734109,"user_tz":0,"elapsed":17449,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}},"outputId":"1f52db13-b20d-4deb-dc50-48990e4e449c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-h6al0_j7\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-h6al0_j7\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"]}]},{"cell_type":"code","source":["import clip\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import torch\n","\n","# Set the device (GPU if available)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"SG5UpIcqO1ko","executionInfo":{"status":"ok","timestamp":1734349743372,"user_tz":0,"elapsed":9267,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Load an Example Image\n","\n","We'll fetch an example image from the web. For instance, let's use an image of a dog.\n"],"metadata":{"id":"kztY35XtPCuf"}},{"cell_type":"code","source":["image_url = \"https://a.travel-assets.com/findyours-php/viewfinder/images/res70/100000/100677-London.jpg\"\n","\n","response = requests.get(image_url)\n","image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n","image\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gzkhffnEC1UeK0TQYkvnvyknJGAPiHM4"},"id":"Q8qEhhrDO8pY","executionInfo":{"status":"ok","timestamp":1734349754463,"user_tz":0,"elapsed":11094,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}},"outputId":"3256a29e-f8d6-49a1-eb6b-6759e087c208"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Load the CLIP Model\n","\n","Load the CLIP model and corresponding tokenizer.\n"],"metadata":{"id":"HgLnOBEnPyAR"}},{"cell_type":"code","source":["model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0HSn9gDPES9","executionInfo":{"status":"ok","timestamp":1734349757705,"user_tz":0,"elapsed":3248,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}},"outputId":"28b8215b-e44b-42a1-fa96-5b68624dd146"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 84.7MiB/s]\n"]}]},{"cell_type":"markdown","source":["## Preprocessing and Extracting Image Embeddings\n","\n","Use the provided `preprocess` function to convert the image into a suitable tensor and then pass it through the model to obtain the image embedding.\n"],"metadata":{"id":"lMa-iZ5EQAcd"}},{"cell_type":"code","source":["image_input = preprocess(image).unsqueeze(0).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image_input)\n","    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n"],"metadata":{"id":"D3CsQwqWP0oN","executionInfo":{"status":"ok","timestamp":1734349758664,"user_tz":0,"elapsed":960,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Define Text Prompts\n","\n","We will define several text prompts and let CLIP determine which one best matches the image:\n","\n","- \"a photo of a dog\"\n","- \"a photo of a cat\"\n","- \"a photo of a bird\"\n","- \"a photo of a computer\"\n","- \"a photo of a car\"\n"],"metadata":{"id":"r8sZh6GiQEkN"}},{"cell_type":"code","source":["text_prompts = [\n","    \"a photo of a bus\",\n","    \"a photo of a taxi\",\n","    \"a photo of a Road\",\n","    \"a photo of a cat\",\n","    \"a photo of a dog\",\n","    \"a photo of a bird\",\n","    \"a photo of a computer\",\n","    \"a photo of a phone\"\n","]\n","\n","text_inputs = clip.tokenize(text_prompts).to(device)\n","\n","with torch.no_grad():\n","    text_features = model.encode_text(text_inputs)\n","    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n"],"metadata":{"id":"VOcYJKIqQCs8","executionInfo":{"status":"ok","timestamp":1734349759311,"user_tz":0,"elapsed":650,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Computing Similarity and Results\n","\n","We'll compute the cosine similarity between the image embedding and each text embedding. The text prompt with the highest similarity score should best describe the image.\n"],"metadata":{"id":"bVBHw8F4QJPm"}},{"cell_type":"code","source":["similarity = (image_features @ text_features.T).cpu().numpy().flatten()\n","\n","# Print all similarity scores along with their corresponding prompt\n","for prompt, score in zip(text_prompts, similarity):\n","    print(f\"Prompt: {prompt} | Similarity score: {score}\")\n","\n","# Identify and print the best match\n","best_match_idx = similarity.argmax()\n","best_prompt = text_prompts[best_match_idx]\n","print(\"\\nThe most similar text prompt to the input image is:\", best_prompt)\n","print(\"Similarity score:\", similarity[best_match_idx])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85uWtIPYQHV8","executionInfo":{"status":"ok","timestamp":1734349759312,"user_tz":0,"elapsed":6,"user":{"displayName":"June Moh Goo","userId":"17495297076246682976"}},"outputId":"6f2b9616-5663-4322-9f2d-e1c0078af689"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: a photo of a bus | Similarity score: 0.238037109375\n","Prompt: a photo of a taxi | Similarity score: 0.216064453125\n","Prompt: a photo of a Road | Similarity score: 0.19775390625\n","Prompt: a photo of a cat | Similarity score: 0.16162109375\n","Prompt: a photo of a dog | Similarity score: 0.15625\n","Prompt: a photo of a bird | Similarity score: 0.171630859375\n","Prompt: a photo of a computer | Similarity score: 0.1766357421875\n","Prompt: a photo of a phone | Similarity score: 0.169189453125\n","\n","The most similar text prompt to the input image is: a photo of a bus\n","Similarity score: 0.238\n"]}]},{"cell_type":"markdown","source":["## Interpretation\n","\n","If the image is of a dog, we expect \"a photo of a dog\" to have the highest similarity score.\n","\n","This completes our simple demonstration of using CLIP for open-vocabulary image classification.\n","\n","## Next Steps\n","\n","- Try different prompts. For instance, try describing specific features of the object in the image.\n","- Experiment with various images and see how CLIP performs.\n","- Explore advanced applications: zero-shot classification for more nuanced concepts, image retrieval, or even image captioning tasks.\n","\n","This concludes the basic CLIP open-vocabulary classification tutorial.\n"],"metadata":{"id":"lMaQdBHJQNIG"}},{"cell_type":"code","source":[],"metadata":{"id":"SddgkZjqQKz5"},"execution_count":null,"outputs":[]}]}